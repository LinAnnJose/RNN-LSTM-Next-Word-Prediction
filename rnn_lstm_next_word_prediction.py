# -*- coding: utf-8 -*-
"""RNN LSTM_Next Word Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oA41z3_nCrUiOnfJ-vewnPS0byE3qZVY
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
import pickle

#To read text file
with open('/content/IndiaUS.txt','r',encoding='utf-8') as myfile:
  mytext = myfile.read()

print(mytext)

mytokenizer=Tokenizer()

#Tokenize to vector format
mytokenizer.fit_on_texts([mytext])
#Find total unique words with index here index starting from 1
#so adding 1 to it to assign/reserve position for the predicted word
total_unique_words=len(mytokenizer.word_index) + 1

total_unique_words #hence total word 598 not 599

mytokenizer.word_index

#Create pickle file for future use, hence it will convert to vector format by next steps.
#So inorder to get the word as prediction we need to convert the corresponding numeric value to word.
#for which this pickle file is required
with open('mytokenizer.pickle','wb') as handle:
  pickle.dump(mytokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)

#N-GRAM SEQUENCE
#bi-gram sequence (2 words)
#tri-gram sequence (3 words)
#Create to ngram sequence(bi gram)
my_input_sequence = []
for line in mytext.split('\n'): #Looping over each line by line
  token_list = mytokenizer.texts_to_sequences([line])[0] #convert each line to sequence
  for i in range(1,len(token_list)):
    #i+1 =bigram
    #i+2 =trigram
    #i+3= quadragram
    n_gram_sequence = token_list[:i+1] # convert this to bi gram format
    #print(n_gram_sequence)
    my_input_sequence.append(n_gram_sequence)

#Find longest line in the text
max_sequence_len = max([len(x) for x in my_input_sequence])
max_sequence_len

min_sequence_len = min([len(x) for x in my_input_sequence])
min_sequence_len

#Padding:To make length of the sequence same by adding 0s at begining or end
#if added at beginning called pre-padding
#If 0s are added at the end post-padding
#Generally used is post padding
#[1,2,3
#1,2
#1] --->[1,2,3,0
#       1,2,0,0
#       1,0,0,0]
#

#Define pre padding
from tensorflow.keras.preprocessing.sequence import pad_sequences
input_sequences = np.array(pad_sequences(my_input_sequence,maxlen=max_sequence_len,padding='pre'))

#Check the padding output by using any one index to see how it works
input_sequences[4]

#Apply supervised learning model
#Splitting x and y
#x is taken as excluding the last element in the input sequence and y is taken as the last element
x=input_sequences[:,:-1]
y=input_sequences[:,-1]

x[4]

y[4]

#One hot encoding
y = np.array(tf.keras.utils.to_categorical(y,num_classes=total_unique_words))

y

#Model creation
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
model = Sequential()
#Embedding means normalize the high dimension data to lower dimension
model.add(Embedding(total_unique_words,100,input_length = max_sequence_len-1))#Here input_length=max_sequence-1 means removing the additionally added index for prediction to get the actual length of the words
model.add(LSTM(128)) #Define no of layers
model.add(Dense(total_unique_words,activation='softmax'))

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
#For multiple class loss finding categorical_crossentropy loss function is being used

model.fit(x,y,epochs = 100,verbose = 1)

model.save('word_predict.h5')